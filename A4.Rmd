---
title: "Assignment 4"
author: "PLSC 21510/31510"
date: "2022"
output: html_document
---

Assigned: November 8, 2022
Due: November 17, 2022

```{r}
# Required packages
library(quanteda)
library(quanteda.textstats)
library(stm)
library(matrixStats)
library(tidyverse)
library(tidytext)
```

# Part 1. Hilary's Emails

In this section we will begin analyzing a collection of emails Hillary Clinton released as part of her (potentially improper) use of a private email server. 

The dataset that we use comes from Kaggle where a team processed an initial release of 7,946 emails to create an easy to read csv file. The complete download is available here:
https://www.kaggle.com/kaggle/hillary-clinton-emails.

We’re going to work with the `Emails.csv` file, which is available in the directory on coursework for this assignment. Run the following code to preprocess the data:

```{r}
# read csv
clinton <- read.csv("Emails.csv") 

# tokenize
clinton.toks <- clinton %>% 
  corpus(text_field = "RawText", docid_field = "Id") %>%
  tokens(split_hyphens = TRUE,
         remove_punct = TRUE,
         remove_numbers = TRUE,
         remove_symbols = TRUE,
         remove_url = TRUE) %>%
  tokens_tolower()  

# make dtm
clinton.dtm <- clinton.toks %>%
  tokens_remove(pattern = stopwords("en")) %>%
  dfm()
```

## 1. Benghazi

You might recall that there was considerable controversy over Hillary Clinton’s role in an incident in Benghazi where a US ambassador and other foreign service officers were killed. We're going to count the number of times Benghazi is used and how it is used in her emails.

### 1.1 

Count the number of times "Benghazi" is used in each email. Print the ID of the email(s) with the highest frequency.

```{r}
# YOUR CODE HERE
```

### 1.2 

Using KWIC, find the 5 words before and after "benghazi" in the emails. Based on your impressions (and not a quantitative analysis, unless you want), when do mentions of Benghazi tend to occur in her email?

```{r}
# YOUR CODE HERE
```

## 2. Sentiment

### 2.1

Using the `bing` dictionary from the `tidytext::get_sentiments` function, calculate the positive sentiment (as a proportion of all pos+neg words) for each email. Print the sentiments of the first 5 emails.

```{r}
# YOUR CODE HERE
```

### 2.2

Regress the positive sentiment score against the number of times Benghazi is mentioned in an email. What do you notice about the relationship?

```{r}
# YOUR CODE HERE
```

### 2.3

Another sentiment dictionary is the 2015 Lexicoder Sentiment Dictionary, which is available in the `data_dictionary_LSD2015` object from quanteda. The dictionary contains both negative/positive as well as "neg_positive" and "neg_negative" phrases 

Read the documentation first! Then, use the dictionary to recalculate the sentiment of the emails. Both "negative" and "neg_positive" frequencies should be counted as "negative, and vice versa for "postive and "neg_negative". 

After you recalculate the positive sentiment score, re-estimate its relationship to "Benghazi" frequency. Did your results change?

```{r}
# YOUR CODE HERE
```

# Part 2: Political Blogs

## 3. K-Means Clustering

For the rest of the assignment, we will analyze political blogs from 2008. We are interested in exploring the themes and topics political commentators address, and the extent to which these themes differ across the liberal-conservative spectrum.

The data we'll be working with is available in the `poliblog5k` object from the `stm` package. It includes a 5000 document sample from CMU 2008 Political Blog Corpus (Eisenstein and Xing 2010). Blog posts come from 6 blogs during the U.S. 2008 Presidential Election. It includes the following variables:

- `rating`: a factor variable giving the partisan affiliation of the blog (based on who they supported for president)
- `day`: the day of the year (1 to 365). All entries are from 2008.
- `blog`: a two digit character code corresponding to the name of the blog. They are: American Thinker (at), Digby (db), Hot Air (ha), Michelle Malkin (mm), Think Progress (tp), Talking Points Memo (tpm)
- `text`: the first 50 characters (rounded to the nearest full word).

Please keep all open-ended responses to, at most, one paragraph.

In this section, you will use the kmeans algorithm to cluster the documents. The code below loads the blogs and conducts the usual pre-processing steps.

```{r}
# import the data
blogs <- poliblog5k.meta
blogs$text <- as.character(blogs$text)

# create DTM
docs <- corpus(blogs, text_field = "text")
dtm <- docs %>%
  tokens(split_hyphens = T,
         remove_punct = TRUE,
         remove_numbers = TRUE,
         remove_symbols = TRUE) %>%
  tokens_tolower(keep_acronyms = F) %>%  
  tokens_remove(pattern = stopwords("en")) %>%
  tokens_wordstem() %>%
  dfm()

# print the dimensions of the DTM
dim(dtm)

# convert to dataframe
dtm <- convert(dtm, to = "data.frame")
dtm$doc_id <- NULL # remove doc_id column
```

### 3.1. 

Normalize the rows of dtm use so that each row sums to 1.

```{r}
# YOUR CODE HERE
```

### 3.2

Assuming that K = 15, apply K-Means to the tweets with the normalized rows. Report the size (i.e., number of observations) in each cluster. Remember to set the seed so that you can reproduce your work!

```{r}
# YOUR CODE HERE
```

### 3.3

Report the top 10 1) "most frequent" words, and 2) "most distinctive" words in each cluster.

```{r}
# YOUR CODE HERE
```

### 3.4

Select one cluster of your choice and read (a sample of) documents from that cluster. Apply a hand label and justify your choice.

```{r}
# YOUR CODE HERE
```

## 4. Structural Topic Model

In this section you will apply a topic model to the `poliblog5k` corpus using the `stm` package. Recall that `stm` requires our data to be formatted a particular way. So we can't rely on our usual DTM.

Luckily, `stm` provides its own preprocessing functions that we can use to get our data in the right format. We've gone ahead and completed those steps for you.

Run the code below to obtain the following objects:
- `docs`: contains the documents to supply to stm
- `vocab` contains the vocab to supply to stm
- `meta` contains the meta information about the blog to supply to the stm.

```{r}
docs <- poliblog5k.docs
vocab <- poliblog5k.voc
meta <- poliblog5k.meta
```

### 4.1 

Using the `stm` function, create a topic model where the prevalence of topics depend on the blog's party affiliation. Set the following options:

- `k = 20` (estiamte 20 topics)
- `init.type = "Spectral"` (a method of initialization that doesn't involve random chance)
- `prevalence = ~rating` (to examine the prevalence of each topic based on blog's partisanship)
- `max.em.its = 15` (allow to algorithm to iterate a maximum of 15 times. This will cut down on run time, although it will provide rough cuts of topics.)

We *highly recommend* you read the help file for the `stm` function before attempting this!

```{r}
# YOUR CODE HERE
```

### 4.2

Using `labelTopics` and the other output from `stm`, assign a hand label to each topic and save these in an object called `labels`.

Then report the prevalence of the topic across the documents. 

```{r}
# YOUR CODE HERE

# labels <- c() # FILL ME OUT
```

### 4.3

Using `estimateEffect` and `plot.estimateEffect` compare how Republicans and Democrats differ in their attention to the topics. On what topics are they most distinct? On what topics are they most similar? 

```{r}
# FILL ME OUT
```

